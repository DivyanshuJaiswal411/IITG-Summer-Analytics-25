{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression (best threshold: 0.67):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Adult       0.91      0.76      0.83       328\n",
      "      Senior       0.32      0.60      0.42        63\n",
      "\n",
      "    accuracy                           0.73       391\n",
      "   macro avg       0.62      0.68      0.62       391\n",
      "weighted avg       0.81      0.73      0.76       391\n",
      "\n",
      "\n",
      "Random Forest (best threshold: 0.35):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Adult       0.89      0.69      0.77       328\n",
      "      Senior       0.25      0.56      0.35        63\n",
      "\n",
      "    accuracy                           0.66       391\n",
      "   macro avg       0.57      0.62      0.56       391\n",
      "weighted avg       0.79      0.66      0.71       391\n",
      "\n",
      "\n",
      "XGBoost (best threshold: 0.26):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Adult       0.89      0.62      0.73       328\n",
      "      Senior       0.24      0.62      0.34        63\n",
      "\n",
      "    accuracy                           0.62       391\n",
      "   macro avg       0.57      0.62      0.54       391\n",
      "weighted avg       0.79      0.62      0.67       391\n",
      "\n",
      "\n",
      "Stacking (best threshold: 0.27):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Adult       0.89      0.69      0.78       328\n",
      "      Senior       0.25      0.54      0.34        63\n",
      "\n",
      "    accuracy                           0.67       391\n",
      "   macro avg       0.57      0.62      0.56       391\n",
      "weighted avg       0.78      0.67      0.71       391\n",
      "\n",
      "\n",
      "Selected model: Logistic Regression with threshold 0.67\n",
      "\n",
      "Top 10 selected feature importances:\n",
      " LBXGLU LBXGLT    0.180165\n",
      "LBXGLT           0.151832\n",
      "BMXBMI LBXGLT    0.116215\n",
      "LBXGLT LBXIN     0.077408\n",
      "BMXBMI           0.073654\n",
      "LBXGLU           0.071186\n",
      "BMXBMI LBXIN     0.070468\n",
      "LBXIN            0.069778\n",
      "BMXBMI LBXGLU    0.067855\n",
      "LBXGLU LBXIN     0.061635\n",
      "dtype: float64\n",
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_data = pd.read_csv('Train_Data.csv')\n",
    "test_data = pd.read_csv('Test_Data.csv')\n",
    "\n",
    "\n",
    "train_data['age_group'] = train_data['age_group'].map({'Adult': 0, 'Senior': 1}).astype('Int64')\n",
    "\n",
    "\n",
    "cols = ['RIAGENDR', 'PAQ605', 'BMXBMI', 'LBXGLU', 'DIQ010', 'LBXGLT', 'LBXIN']\n",
    "for col in cols:\n",
    "    if train_data[col].dtype in ['float64', 'int64']:\n",
    "        train_data[col] = train_data[col].fillna(train_data[col].median())\n",
    "    else:\n",
    "        train_data[col] = train_data[col].fillna(train_data[col].mode()[0])\n",
    "train_data = train_data.dropna(subset=['age_group'])\n",
    "\n",
    "for col in cols:\n",
    "    if test_data[col].dtype in ['float64', 'int64']:\n",
    "        test_data[col] = test_data[col].fillna(train_data[col].median())\n",
    "    else:\n",
    "        test_data[col] = test_data[col].fillna(train_data[col].mode()[0])\n",
    "\n",
    "num_cols = ['BMXBMI', 'LBXGLU', 'LBXGLT', 'LBXIN']\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "poly_features_train = poly.fit_transform(train_data[num_cols])\n",
    "poly_features_test = poly.transform(test_data[num_cols])\n",
    "poly_feature_names = poly.get_feature_names_out(num_cols)\n",
    "\n",
    "base_features = ['RIAGENDR', 'PAQ605', 'DIQ010']\n",
    "feature_cols = base_features + list(poly_feature_names)\n",
    "\n",
    "X_train_full = np.concatenate([train_data[base_features].values, poly_features_train], axis=1)\n",
    "X_test_full  = np.concatenate([test_data[base_features].values, poly_features_test], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_full_scaled = scaler.fit_transform(X_train_full)\n",
    "X_test_full_scaled  = scaler.transform(X_test_full)\n",
    "\n",
    "y = train_data['age_group'].astype(int)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_res, y_res = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "rf_selector.fit(X_res, y_res)\n",
    "thresholds = ['mean', 'median', 0.01, 0.02, 0.03]\n",
    "best_fs_f1 = 0\n",
    "for thresh in thresholds:\n",
    "    selector = SelectFromModel(rf_selector, prefit=True, threshold=thresh)\n",
    "    X_res_sel = selector.transform(X_res)\n",
    "    X_val_sel = selector.transform(X_val)\n",
    "    # Quick check with simple logistic regression\n",
    "    lr_fs = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "    lr_fs.fit(X_res_sel, y_res)\n",
    "    y_pred = lr_fs.predict(X_val_sel)\n",
    "    f1 = f1_score(y_val, y_pred, pos_label=1)\n",
    "    if f1 > best_fs_f1:\n",
    "        best_fs_f1 = f1\n",
    "        best_selector = selector\n",
    "        best_X_res_sel = X_res_sel\n",
    "        best_X_val_sel = X_val_sel\n",
    "        best_X_test_sel = selector.transform(X_test_full_scaled)\n",
    "        best_selected_features = np.array(feature_cols)[selector.get_support()]\n",
    "\n",
    "X_res_sel = best_X_res_sel\n",
    "X_val_sel = best_X_val_sel\n",
    "X_test_sel = best_X_test_sel\n",
    "selected_features = best_selected_features\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "lr_params = {'C': np.logspace(-2, 2, 10), 'solver': ['lbfgs', 'liblinear']}\n",
    "lr_search = RandomizedSearchCV(LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42),\n",
    "                               lr_params, n_iter=5, scoring=scorer, cv=cv, random_state=42)\n",
    "lr_search.fit(X_res_sel, y_res)\n",
    "lr = CalibratedClassifierCV(lr_search.best_estimator_, method='sigmoid', cv=3)\n",
    "\n",
    "rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 7, None], 'min_samples_split': [2, 5, 10]}\n",
    "rf_search = RandomizedSearchCV(RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "                               rf_params, n_iter=5, scoring=scorer, cv=cv, random_state=42)\n",
    "rf_search.fit(X_res_sel, y_res)\n",
    "rf = CalibratedClassifierCV(rf_search.best_estimator_, method='isotonic', cv=3)\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "scale_pos_weight = (y_res == 0).sum() / (y_res == 1).sum()\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    XGBClassifier(eval_metric='logloss', scale_pos_weight=scale_pos_weight, random_state=42),\n",
    "    xgb_params, n_iter=5, scoring=scorer, cv=cv, random_state=42\n",
    ")\n",
    "xgb_search.fit(X_res_sel, y_res)\n",
    "xgb = CalibratedClassifierCV(xgb_search.best_estimator_, method='sigmoid', cv=3)\n",
    "\n",
    "lr.fit(X_res_sel, y_res)\n",
    "rf.fit(X_res_sel, y_res)\n",
    "xgb.fit(X_res_sel, y_res)\n",
    "\n",
    "stack_estimators = [\n",
    "    ('lr', lr),\n",
    "    ('rf', rf),\n",
    "    ('xgb', xgb)\n",
    "]\n",
    "meta_params = {'C': np.logspace(-2, 2, 10), 'solver': ['lbfgs', 'liblinear']}\n",
    "meta_search = RandomizedSearchCV(LogisticRegression(max_iter=1000), meta_params, n_iter=5, scoring=scorer, cv=cv, random_state=42)\n",
    "meta_search.fit(X_val_sel, y_val)\n",
    "stack = StackingClassifier(\n",
    "    estimators=stack_estimators,\n",
    "    final_estimator=meta_search.best_estimator_,\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "stack.fit(X_res_sel, y_res)\n",
    "\n",
    "def best_threshold(model, X_val, y_val):\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    thresholds = np.arange(0.05, 0.91, 0.01)\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0.5\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_proba >= t).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred, pos_label=1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = t\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "models = {'Logistic Regression': lr, 'Random Forest': rf, 'XGBoost': xgb, 'Stacking': stack}\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    thresh, f1 = best_threshold(model, X_val_sel, y_val)\n",
    "    y_proba = model.predict_proba(X_val_sel)[:, 1]\n",
    "    y_pred = (y_proba >= thresh).astype(int)\n",
    "    print(f\"\\n{name} (best threshold: {thresh:.2f}):\")\n",
    "    print(classification_report(y_val, y_pred, target_names=['Adult', 'Senior']))\n",
    "    results[name] = (f1, thresh)\n",
    "\n",
    "best_model_name = max(results, key=lambda k: results[k][0])\n",
    "best_model = models[best_model_name]\n",
    "best_thresh = results[best_model_name][1]\n",
    "print(f\"\\nSelected model: {best_model_name} with threshold {best_thresh:.2f}\")\n",
    "\n",
    "importances = np.mean(\n",
    "    [clf.estimator.feature_importances_ for clf in rf.calibrated_classifiers_], axis=0\n",
    ")\n",
    "feat_imp = pd.Series(importances, index=selected_features).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 selected feature importances:\\n\", feat_imp.head(10))\n",
    "\n",
    "\n",
    "y_test_proba = best_model.predict_proba(X_test_sel)[:, 1]\n",
    "y_test_pred = (y_test_proba >= best_thresh).astype(int)\n",
    "submission = pd.DataFrame({'age_group': y_test_pred.astype(int)})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
